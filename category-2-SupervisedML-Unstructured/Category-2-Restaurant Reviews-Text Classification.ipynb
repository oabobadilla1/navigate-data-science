{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category-2-Supervised Machine Learning on Unstructured data\n",
    "> Dataset consists of a set of reviews written by customers and the corresponding label indicating whether they 'Liked' the experience or not. The objective of the learning program is to predict the label 'Liked' based on the text review. So this is a text classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 1 - Import relevant libraries **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re,nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 2 - Readin the dataset into pandas dataframe **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "reviews_original = pd.read_csv('./0.datasets/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "print(reviews_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_original.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_reviews = reviews_original.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 3 - Text Preprocessing **\n",
    "> Remove special characters\n",
    "\n",
    "> Make lowercase\n",
    "\n",
    "> Stem the words\n",
    "\n",
    "> Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_corpus = []\n",
    "for i in range(0, num_reviews):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', reviews_original['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    review_corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 4: Create the bag of words by using Count Vectorizer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "reviews_bow = cv.fit_transform(review_corpus)\n",
    "vocab_bow = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (1000, 1565)\n",
      "Amount of Non-Zero occurences:  5372\n",
      "sparsity: 0.34325878594249204\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix: ', reviews_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', reviews_bow.nnz)\n",
    "\n",
    "sparsity = (100.0 * reviews_bow.nnz / (reviews_bow.shape[0] * reviews_bow.shape[1]))\n",
    "print('sparsity: {}'.format(sparsity))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 5: Create training & validation datasets and run machine learning models on bag of words **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_bow = reviews_bow.toarray()\n",
    "y_bow = reviews_original.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_bow, y_bow, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting 3 Algorithms to the Training set\n",
    "review_gaussian_nb_bow = GaussianNB()\n",
    "review_log_bow = LogisticRegression()\n",
    "review_sgd_bow = SGDClassifier()\n",
    "\n",
    "review_gaussian_nb_bow.fit(X_train, y_train)\n",
    "review_log_bow.fit(X_train, y_train)\n",
    "review_sgd_bow.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Validation set results\n",
    "predictions_gaussian_nb_bow = review_gaussian_nb_bow.predict(X_val)\n",
    "predictions_log_bow = review_log_bow.predict(X_val)\n",
    "predictions_sgd_bow = review_sgd_bow.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 6: Compare results from different algorithms **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy - BOW: 0.73\n",
      "Logistic Regression Accuracy - BOW: 0.71\n",
      "SGD Accuracy - BOW: 0.725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Naive Bayes Accuracy - BOW:\",accuracy_score(y_val,predictions_gaussian_nb_bow))\n",
    "print(\"Logistic Regression Accuracy - BOW:\",accuracy_score(y_val,predictions_log_bow))\n",
    "print(\"SGD Accuracy - BOW:\",accuracy_score(y_val,predictions_sgd_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 7: Create TFIDF scores for each token in the bag of words ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1565)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "reviews_tfidf = tfidf.fit_transform(reviews_bow)\n",
    "print(reviews_tfidf.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 8: Create training & validation datasets and run machine learning models on TFIDF **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tfidf = reviews_tfidf.toarray()\n",
    "y_tfidf = reviews_original.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tfidf, y_tfidf, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting 3 Algorithms to the Training set\n",
    "review_gaussian_nb_tfidf = GaussianNB()\n",
    "review_log_tfidf = LogisticRegression()\n",
    "review_sgd_tfidf = SGDClassifier()\n",
    "\n",
    "review_gaussian_nb_tfidf.fit(X_train, y_train)\n",
    "review_log_tfidf.fit(X_train, y_train)\n",
    "review_sgd_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Validation set results\n",
    "predictions_gaussian_nb_tfidf = review_gaussian_nb_tfidf.predict(X_val)\n",
    "predictions_log_tfidf = review_log_tfidf.predict(X_val)\n",
    "predictions_sgd_tfidf = review_sgd_tfidf.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 9: Compare results from different algorithms for TFIDF **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy - TFIDF: 0.72\n",
      "Logistic Regression Accuracy - TFIDF: 0.755\n",
      "SGD Accuracy - TFIDF: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Naive Bayes Accuracy - TFIDF:\",accuracy_score(y_val,predictions_gaussian_nb_tfidf))\n",
    "print(\"Logistic Regression Accuracy - TFIDF:\",accuracy_score(y_val,predictions_log_tfidf))\n",
    "print(\"SGD Accuracy - TFIDF:\",accuracy_score(y_val,predictions_sgd_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 10: Implement Grid Search to find the right hyper-parameters **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pprint import pprint\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a pipeline combining a text feature extractor with a simple classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__n_iter': (10, 50, 80),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_for_param_tuning = review_corpus\n",
    "y_for_param_tuning = reviews_original.Liked.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (1e-05, 1e-06),\n",
      " 'clf__n_iter': (10, 50, 80),\n",
      " 'clf__penalty': ('l2', 'elasticnet'),\n",
      " 'tfidf__norm': ('l1', 'l2'),\n",
      " 'tfidf__use_idf': (True, False),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__max_features': (None, 5000, 10000, 50000),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 1152 candidates, totalling 3456 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 717 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1417 tasks      | elapsed:   44.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2317 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3417 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3449 out of 3456 | elapsed:  1.7min remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 3456 out of 3456 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 103.727s\n",
      "\n",
      "Best score: 0.773\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__n_iter: 10\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\ttfidf__norm: 'l1'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: 50000\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_for_param_tuning, y_for_param_tuning)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 11 - Run machine learning algorithms using the hyper-parameters found in the previous step **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 50000,max_df=0.75,ngram_range=(1,2))\n",
    "reviews_bow = cv.fit_transform(review_corpus)\n",
    "vocab_bow = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5634)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(norm = 'l1',use_idf='True')\n",
    "reviews_tfidf = tfidf.fit_transform(reviews_bow)\n",
    "print(reviews_tfidf.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = reviews_tfidf.toarray()\n",
    "y = reviews_original.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karthikeyan\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Accuracy after tuning: 0.75\n"
     ]
    }
   ],
   "source": [
    "review_sgd_model = SGDClassifier(penalty='elasticnet',alpha=0.00001,n_iter=10)\n",
    "review_sgd_model = review_sgd_model.fit(X=X_train, y=y_train)\n",
    "predictions_sgd_tfidf = review_sgd_model.predict(X_val)\n",
    "print(\"SGD Accuracy after tuning:\",accuracy_score(y_val,predictions_sgd_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Conclusion: Accuracy of the model has improved after tuning the hyper-parameters **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
