{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category-2-Supervised Machine Learning on Unstructured data\n",
    "> Dataset consists of a set of reviews written by customers and the corresponding label indicating whether they 'Liked' the experience or not. The objective of the learning program is to predict the label 'Liked' based on the text review. So this is a text classification problem.\n",
    "> This version utilizes sklearn pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 1 - Import relevant libraries **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "import string\n",
    "import re,nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# Download just once\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 2 - Reading the dataset into pandas dataframe **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "reviews_original = pd.read_csv('./0.datasets/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "print(reviews_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_original.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "num_reviews = reviews_original.shape[0]\n",
    "print(num_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 3 - Text Preprocessing **\n",
    "> Remove special characters\n",
    "\n",
    "> Make lowercase\n",
    "\n",
    "> Stem the words\n",
    "\n",
    "> Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to encapsulate preprocessing, to make it easy to replicate on submission data\n",
    "def processing(df,col='text',remove_stop_words='Yes',treatment='lemmatize'):\n",
    "    num_reviews = df.shape[0]\n",
    "    #lowering and removing punctuation\n",
    "    df['processed'] = df[col].apply(lambda x: re.sub(r'[^\\w\\s]','', x.lower()))\n",
    "    \n",
    "    #numerical feature engineering\n",
    "    #total length of sentence\n",
    "    df['length'] = df['processed'].apply(lambda x: len(x))\n",
    "    #get number of words\n",
    "    df['words'] = df['processed'].apply(lambda x: len(x.split(' ')))\n",
    "    df['words_not_stopword'] = df['processed'].apply(lambda x: len([t for t in x.split(' ') if t not in stopWords]))\n",
    "    #get the average word length\n",
    "    df['avg_word_length'] = df['processed'].apply(lambda x: np.mean([len(t) for t in x.split(' ') if t not in stopWords]) if len([len(t) for t in x.split(' ') if t not in stopWords]) > 0 else 0)\n",
    "    #get the average word length\n",
    "    df['commas'] = df[col].apply(lambda x: x.count(','))\n",
    "   \n",
    "    if remove_stop_words==\"Yes\":\n",
    "        df['processed'] = df['processed'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopWords)]))\n",
    "\n",
    "    if treatment==\"stemming\":\n",
    "        df['processed'] = df['processed'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in (stopWords)]))\n",
    "    elif treatment==\"lemmatize\":\n",
    "        df['processed'] = df['processed'].apply(lambda x: ' '.join([lem.lemmatize(word) for word in x.split() if word not in (stopWords)]))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "      <th>processed</th>\n",
       "      <th>length</th>\n",
       "      <th>words</th>\n",
       "      <th>words_not_stopword</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>commas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>wow loved place</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>crust good</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>tasty texture nasty</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>stopped late may bank holiday rick steve recom...</td>\n",
       "      <td>86</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5.888889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>selection menu great price</td>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked  \\\n",
       "0                           Wow... Loved this place.      1   \n",
       "1                                 Crust is not good.      0   \n",
       "2          Not tasty and the texture was just nasty.      0   \n",
       "3  Stopped by during the late May bank holiday of...      1   \n",
       "4  The selection on the menu was great and so wer...      1   \n",
       "\n",
       "                                           processed  length  words  \\\n",
       "0                                    wow loved place      20      4   \n",
       "1                                         crust good      17      4   \n",
       "2                                tasty texture nasty      40      8   \n",
       "3  stopped late may bank holiday rick steve recom...      86     15   \n",
       "4                         selection menu great price      58     12   \n",
       "\n",
       "   words_not_stopword  avg_word_length  commas  \n",
       "0                   3         4.333333       0  \n",
       "1                   2         4.500000       0  \n",
       "2                   3         5.666667       0  \n",
       "3                   9         5.888889       0  \n",
       "4                   4         6.000000       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_processed = processing(reviews_original,col='Review',remove_stop_words='Yes',treatment='lemmatize')\n",
    "reviews_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_corpus = reviews_processed['processed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 4: Initial Analysis - Bag of Words using CountVectorizer & Tfidf transformer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "reviews_bow = cv.fit_transform(review_corpus)\n",
    "vocab_bow = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (1000, 1829)\n",
      "Amount of Non-Zero occurences:  5532\n",
      "sparsity: 0.3024603608529251\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix: ', reviews_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', reviews_bow.nnz)\n",
    "\n",
    "sparsity = (100.0 * reviews_bow.nnz / (reviews_bow.shape[0] * reviews_bow.shape[1]))\n",
    "print('sparsity: {}'.format(sparsity))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1829)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "reviews_tfidf = tfidf.fit_transform(reviews_bow)\n",
    "print(reviews_tfidf.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note: You can continue to fit classifiers on Bag of Words & TFIDF values as explained in the previous version of the same notebook. In this notebook, am going to show how to use pipelines for fitting the classifer models **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Creating the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 5.1: Creating custom transformers (as required) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    \n",
    "class DenseTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 5.2: Split into train & validation set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the features & target\n",
    "features = [c for c in reviews_processed.columns.values if c not in ['Review','Liked']]\n",
    "numeric_features= [c for c in reviews_processed.columns.values if c  not in ['Review','Liked','processed']]\n",
    "target = 'Liked'\n",
    "\n",
    "X = reviews_processed[features]\n",
    "y = reviews_processed[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed</th>\n",
       "      <th>length</th>\n",
       "      <th>words</th>\n",
       "      <th>words_not_stopword</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>commas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>havent gone go</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>try airport experience tasty food speedy frien...</td>\n",
       "      <td>81</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>restaurant clean family restaurant feel</td>\n",
       "      <td>67</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>personally love hummus pita baklava falafel ba...</td>\n",
       "      <td>106</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>6.50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>come hungry leave happy stuffed</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             processed  length  words  \\\n",
       "521                                     havent gone go      30      7   \n",
       "737  try airport experience tasty food speedy frien...      81     14   \n",
       "740            restaurant clean family restaurant feel      67     13   \n",
       "660  personally love hummus pita baklava falafel ba...     106     18   \n",
       "411                    come hungry leave happy stuffed      35      6   \n",
       "\n",
       "     words_not_stopword  avg_word_length  commas  \n",
       "521                   3             4.00       0  \n",
       "737                   8             6.25       1  \n",
       "740                   5             7.00       0  \n",
       "660                  10             6.50       3  \n",
       "411                   5             5.40       1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 5.3: Create separate pipeline for each feature **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_tfidf = Pipeline([\n",
    "                ('selector', TextSelector(key = 'processed')),\n",
    "                ('tfidf', TfidfVectorizer(stop_words='english'))\n",
    "            ])\n",
    "\n",
    "text_cv = Pipeline([\n",
    "                ('selector', TextSelector(key='processed')),\n",
    "                ('cv', CountVectorizer())\n",
    "            ])\n",
    "\n",
    "length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "words =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "words_not_stopword =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words_not_stopword')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "avg_word_length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='avg_word_length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "commas =  Pipeline([\n",
    "                ('selector', NumberSelector(key='commas')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 5.4: Combine all features using FeatureUnion & check whether preprocessing makes sense **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = FeatureUnion([('text_tfidf', text_tfidf),\n",
    "                      ('text_cv', text_cv),\n",
    "                      ('length', length),\n",
    "                      ('words', words),\n",
    "                      ('words_not_stopword', words_not_stopword),\n",
    "                      ('avg_word_length', avg_word_length),\n",
    "                      ('commas', commas)\n",
    "                     ])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3079</th>\n",
       "      <th>3080</th>\n",
       "      <th>3081</th>\n",
       "      <th>3082</th>\n",
       "      <th>3083</th>\n",
       "      <th>3084</th>\n",
       "      <th>3085</th>\n",
       "      <th>3086</th>\n",
       "      <th>3087</th>\n",
       "      <th>3088</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.774591</td>\n",
       "      <td>-0.791911</td>\n",
       "      <td>-0.844648</td>\n",
       "      <td>0.255891</td>\n",
       "      <td>-0.577461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.052362</td>\n",
       "      <td>-0.472511</td>\n",
       "      <td>-0.237806</td>\n",
       "      <td>2.256122</td>\n",
       "      <td>-0.577461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.774591</td>\n",
       "      <td>-0.632211</td>\n",
       "      <td>-0.237806</td>\n",
       "      <td>-1.410968</td>\n",
       "      <td>-0.577461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073243</td>\n",
       "      <td>-0.153112</td>\n",
       "      <td>0.065615</td>\n",
       "      <td>0.533701</td>\n",
       "      <td>-0.577461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.334973</td>\n",
       "      <td>-0.312812</td>\n",
       "      <td>-0.844648</td>\n",
       "      <td>0.811511</td>\n",
       "      <td>-0.577461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3089 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9       ...     3079  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    ...      0.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    ...      0.0   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    ...      0.0   \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    ...      0.0   \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    ...      0.0   \n",
       "\n",
       "   3080  3081  3082  3083      3084      3085      3086      3087      3088  \n",
       "0   0.0   0.0   0.0   0.0 -0.774591 -0.791911 -0.844648  0.255891 -0.577461  \n",
       "1   0.0   0.0   0.0   0.0 -0.052362 -0.472511 -0.237806  2.256122 -0.577461  \n",
       "2   0.0   0.0   0.0   0.0 -0.774591 -0.632211 -0.237806 -1.410968 -0.577461  \n",
       "3   0.0   0.0   0.0   0.0  0.073243 -0.153112  0.065615  0.533701 -0.577461  \n",
       "4   0.0   0.0   0.0   0.0 -0.334973 -0.312812 -0.844648  0.811511 -0.577461  \n",
       "\n",
       "[5 rows x 3089 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.DataFrame(feature_processing.fit_transform(X_train,y_train).todense())\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3079</th>\n",
       "      <th>3080</th>\n",
       "      <th>3081</th>\n",
       "      <th>3082</th>\n",
       "      <th>3083</th>\n",
       "      <th>3084</th>\n",
       "      <th>3085</th>\n",
       "      <th>3086</th>\n",
       "      <th>3087</th>\n",
       "      <th>3088</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.837394</td>\n",
       "      <td>-0.632211</td>\n",
       "      <td>-0.844648</td>\n",
       "      <td>-1.410968</td>\n",
       "      <td>-0.577461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.764072</td>\n",
       "      <td>0.485687</td>\n",
       "      <td>0.672456</td>\n",
       "      <td>0.464248</td>\n",
       "      <td>0.988535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324454</td>\n",
       "      <td>0.325987</td>\n",
       "      <td>-0.237806</td>\n",
       "      <td>1.089320</td>\n",
       "      <td>-0.577461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.549104</td>\n",
       "      <td>1.124485</td>\n",
       "      <td>1.279298</td>\n",
       "      <td>0.672606</td>\n",
       "      <td>4.120526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.680387</td>\n",
       "      <td>-0.791911</td>\n",
       "      <td>-0.237806</td>\n",
       "      <td>-0.244167</td>\n",
       "      <td>0.988535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3089 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9       ...     3079  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    ...      0.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    ...      0.0   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    ...      0.0   \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    ...      0.0   \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    ...      0.0   \n",
       "\n",
       "   3080  3081  3082  3083      3084      3085      3086      3087      3088  \n",
       "0   0.0   0.0   0.0   0.0 -0.837394 -0.632211 -0.844648 -1.410968 -0.577461  \n",
       "1   0.0   0.0   0.0   0.0  0.764072  0.485687  0.672456  0.464248  0.988535  \n",
       "2   0.0   0.0   0.0   0.0  0.324454  0.325987 -0.237806  1.089320 -0.577461  \n",
       "3   0.0   0.0   0.0   0.0  1.549104  1.124485  1.279298  0.672606  4.120526  \n",
       "4   0.0   0.0   0.0   0.0 -0.680387 -0.791911 -0.237806 -0.244167  0.988535  \n",
       "\n",
       "[5 rows x 3089 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df_val = pd.DataFrame(feature_processing.transform(X_val).todense())\n",
    "text_df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 5.5: Compare different classifiers using KFold cross validation ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(),\n",
    "    XGBClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('todense',DenseTransformer()),\n",
    "    ('classifier', classifiers[0])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "GaussianNB(priors=None)\n",
      "----------------------\n",
      "[ 0.66   0.71   0.685  0.75   0.68 ]\n",
      "0.697\n",
      "----------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "----------------------\n",
      "[ 0.77   0.78   0.785  0.785  0.8  ]\n",
      "0.784\n",
      "----------------------\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "----------------------\n",
      "[ 0.735  0.74   0.755  0.785  0.765]\n",
      "0.756\n",
      "----------------------\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "----------------------\n",
      "[ 0.725  0.695  0.735  0.7    0.745]\n",
      "0.72\n"
     ]
    }
   ],
   "source": [
    "if 1==1:\n",
    "    from sklearn import model_selection\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    scoring = 'accuracy'\n",
    "    #scoring = 'roc_auc'\n",
    "    #scoring = \"f1_macro\"\n",
    "    #scoring = 'neg_log_loss'\n",
    "\n",
    "    n_splits=5\n",
    "    seed = 10\n",
    "\n",
    "    #kfold = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    kfold = model_selection.StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    for clf in classifiers:\n",
    "        model_pipeline.set_params(classifier=clf)\n",
    "        scores = cross_val_score(model_pipeline, X, y, scoring=scoring, cv=kfold)\n",
    "        print('----------------------')\n",
    "        print(str(clf))\n",
    "        print('----------------------')\n",
    "        print(scores)\n",
    "        print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 5.6: Select the best classifier for hyperparameter tuning using GridSearch **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model_pipeline = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('todense',DenseTransformer()),\n",
    "    ('lr_classifier', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('text_tfidf', Pipeline(steps=[('selector', TextSelector(key='processed')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "    ...logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1))])\n"
     ]
    }
   ],
   "source": [
    "# use the pipeline object as you would a regular classifier\n",
    "lr_model_pipeline.fit(X_train,y_train)\n",
    "print(model_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance before hyperparameter tuning: 0.78\n"
     ]
    }
   ],
   "source": [
    "y_preds = lr_model_pipeline.predict(X_val)\n",
    "print(\"Performance before hyperparameter tuning: %0.2f\" %accuracy_score(y_val,y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features',\n",
       " 'features__avg_word_length',\n",
       " 'features__avg_word_length__selector',\n",
       " 'features__avg_word_length__selector__key',\n",
       " 'features__avg_word_length__standard',\n",
       " 'features__avg_word_length__standard__copy',\n",
       " 'features__avg_word_length__standard__with_mean',\n",
       " 'features__avg_word_length__standard__with_std',\n",
       " 'features__avg_word_length__steps',\n",
       " 'features__commas',\n",
       " 'features__commas__selector',\n",
       " 'features__commas__selector__key',\n",
       " 'features__commas__standard',\n",
       " 'features__commas__standard__copy',\n",
       " 'features__commas__standard__with_mean',\n",
       " 'features__commas__standard__with_std',\n",
       " 'features__commas__steps',\n",
       " 'features__length',\n",
       " 'features__length__selector',\n",
       " 'features__length__selector__key',\n",
       " 'features__length__standard',\n",
       " 'features__length__standard__copy',\n",
       " 'features__length__standard__with_mean',\n",
       " 'features__length__standard__with_std',\n",
       " 'features__length__steps',\n",
       " 'features__n_jobs',\n",
       " 'features__text_cv',\n",
       " 'features__text_cv__cv',\n",
       " 'features__text_cv__cv__analyzer',\n",
       " 'features__text_cv__cv__binary',\n",
       " 'features__text_cv__cv__decode_error',\n",
       " 'features__text_cv__cv__dtype',\n",
       " 'features__text_cv__cv__encoding',\n",
       " 'features__text_cv__cv__input',\n",
       " 'features__text_cv__cv__lowercase',\n",
       " 'features__text_cv__cv__max_df',\n",
       " 'features__text_cv__cv__max_features',\n",
       " 'features__text_cv__cv__min_df',\n",
       " 'features__text_cv__cv__ngram_range',\n",
       " 'features__text_cv__cv__preprocessor',\n",
       " 'features__text_cv__cv__stop_words',\n",
       " 'features__text_cv__cv__strip_accents',\n",
       " 'features__text_cv__cv__token_pattern',\n",
       " 'features__text_cv__cv__tokenizer',\n",
       " 'features__text_cv__cv__vocabulary',\n",
       " 'features__text_cv__selector',\n",
       " 'features__text_cv__selector__key',\n",
       " 'features__text_cv__steps',\n",
       " 'features__text_tfidf',\n",
       " 'features__text_tfidf__selector',\n",
       " 'features__text_tfidf__selector__key',\n",
       " 'features__text_tfidf__steps',\n",
       " 'features__text_tfidf__tfidf',\n",
       " 'features__text_tfidf__tfidf__analyzer',\n",
       " 'features__text_tfidf__tfidf__binary',\n",
       " 'features__text_tfidf__tfidf__decode_error',\n",
       " 'features__text_tfidf__tfidf__dtype',\n",
       " 'features__text_tfidf__tfidf__encoding',\n",
       " 'features__text_tfidf__tfidf__input',\n",
       " 'features__text_tfidf__tfidf__lowercase',\n",
       " 'features__text_tfidf__tfidf__max_df',\n",
       " 'features__text_tfidf__tfidf__max_features',\n",
       " 'features__text_tfidf__tfidf__min_df',\n",
       " 'features__text_tfidf__tfidf__ngram_range',\n",
       " 'features__text_tfidf__tfidf__norm',\n",
       " 'features__text_tfidf__tfidf__preprocessor',\n",
       " 'features__text_tfidf__tfidf__smooth_idf',\n",
       " 'features__text_tfidf__tfidf__stop_words',\n",
       " 'features__text_tfidf__tfidf__strip_accents',\n",
       " 'features__text_tfidf__tfidf__sublinear_tf',\n",
       " 'features__text_tfidf__tfidf__token_pattern',\n",
       " 'features__text_tfidf__tfidf__tokenizer',\n",
       " 'features__text_tfidf__tfidf__use_idf',\n",
       " 'features__text_tfidf__tfidf__vocabulary',\n",
       " 'features__transformer_list',\n",
       " 'features__transformer_weights',\n",
       " 'features__words',\n",
       " 'features__words__selector',\n",
       " 'features__words__selector__key',\n",
       " 'features__words__standard',\n",
       " 'features__words__standard__copy',\n",
       " 'features__words__standard__with_mean',\n",
       " 'features__words__standard__with_std',\n",
       " 'features__words__steps',\n",
       " 'features__words_not_stopword',\n",
       " 'features__words_not_stopword__selector',\n",
       " 'features__words_not_stopword__selector__key',\n",
       " 'features__words_not_stopword__standard',\n",
       " 'features__words_not_stopword__standard__copy',\n",
       " 'features__words_not_stopword__standard__with_mean',\n",
       " 'features__words_not_stopword__standard__with_std',\n",
       " 'features__words_not_stopword__steps',\n",
       " 'lr_classifier',\n",
       " 'lr_classifier__C',\n",
       " 'lr_classifier__class_weight',\n",
       " 'lr_classifier__dual',\n",
       " 'lr_classifier__fit_intercept',\n",
       " 'lr_classifier__intercept_scaling',\n",
       " 'lr_classifier__max_iter',\n",
       " 'lr_classifier__multi_class',\n",
       " 'lr_classifier__n_jobs',\n",
       " 'lr_classifier__penalty',\n",
       " 'lr_classifier__random_state',\n",
       " 'lr_classifier__solver',\n",
       " 'lr_classifier__tol',\n",
       " 'lr_classifier__verbose',\n",
       " 'lr_classifier__warm_start',\n",
       " 'steps',\n",
       " 'todense']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all hyper-parameters for the pipeline object\n",
    "sorted(lr_model_pipeline.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['features', 'todense', 'lr_classifier']\n",
      "parameters:\n",
      "{'features__text_tfidf__tfidf__ngram_range': [(1, 1), (1, 2)],\n",
      " 'lr_classifier__C': [50, 70],\n",
      " 'lr_classifier__penalty': ['l1', 'l2']}\n",
      "done in 6.935s\n",
      "\n",
      "Best score: 0.780\n",
      "Best parameters set:\n",
      "\tfeatures__text_tfidf__tfidf__ngram_range: (1, 2)\n",
      "\tlr_classifier__C: 50\n",
      "\tlr_classifier__penalty: 'l2'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameters = { #'features__text_tfidf__tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "                    'features__text_tfidf__tfidf__ngram_range': [(1,1), (1,2)],\n",
    "                    #'features__text_tfidf__tfidf__use_idf': [True, False],\n",
    "                    #'features__text_tfidf__tfidf__norm': ['l1','l2'],\n",
    "                    #'features__text_cv__cv__max_features': [None, 1000, 3000],\n",
    "                    'lr_classifier__C': [50, 70],\n",
    "                    'lr_classifier__penalty' : ['l1','l2']\n",
    "                  }\n",
    "\n",
    "grid_search = GridSearchCV(lr_model_pipeline, hyperparameters, cv=5)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in lr_model_pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(hyperparameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(hyperparameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "\n",
    "# Fit and tune model\n",
    "#clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'features__text_tfidf__tfidf__ngram_range': (1, 2),\n",
       " 'lr_classifier__C': 50,\n",
       " 'lr_classifier__penalty': 'l2'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best parameters are:\")\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 5.7: Refit on entire training set using best parameters obtained used GridSearch **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n"
     ]
    }
   ],
   "source": [
    "#refitting on entire training data using best settings\n",
    "grid_search.refit\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 5.8: Predict on Test Set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance after hyperparameter tuning: 0.80\n"
     ]
    }
   ],
   "source": [
    "preds = grid_search.predict(X_val)\n",
    "probs = grid_search.predict_proba(X_val)\n",
    "print(\"Performance after hyperparameter tuning: %0.2f\" %accuracy_score(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Conclusion: Accuracy of the model has improved after tuning the hyper-parameters **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
